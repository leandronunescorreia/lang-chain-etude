{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsXAs2gcIpbC"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZX50cNFOFBt"
   },
   "source": [
    " # Evaluate LangChain | Gen AI Evaluation SDK Tutorial\n",
    "\n",
    " <table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_langchain_chains.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fevaluation%2Fevaluate_langchain_chains.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/evaluation/evaluate_langchain_chains.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_langchain_chains.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usd0d_LiOFBt"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Elia Secchi](https://github.com/eliasecchig) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjDmmmDaOFBt"
   },
   "source": [
    "## Overview\n",
    "\n",
    "With this tutorial, you learn how to evaluate the performance of a conversational LangChain chain using the *Vertex AI Python SDK for Gen AI Evaluation Service*. The notebook utilizes a dummy chatbot designed to provide recipe suggestions.\n",
    "\n",
    "The tutorial goes trough:\n",
    "1. Data preparation\n",
    "2. Setting up the LangChain chain\n",
    "3. Set-up a custom metric\n",
    "4. Run evaluation with a combination of custom metrics and built-in metrics.\n",
    "5. Log results into an experiment run and analyze different runs.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-OcPSC8_FUX"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7Jso8-FO4N8"
   },
   "source": [
    "### Install Vertex AI SDK for Rapid Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUat7NRq5JDC"
   },
   "outputs": [],
   "source": [
    "# %pip install --quiet --upgrade nest_asyncio\n",
    "# %pip install --upgrade --user --quiet langchain-core langchain-google-vertexai langchain\n",
    "# %pip install --upgrade --user --quiet \"google-cloud-aiplatform[evaluation]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# import vertexai\n",
    "\n",
    "# vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvhI92xhQTzk"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qP4ihOCkEBje"
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import json\n",
    "import logging\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Main\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# General\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDmLyf-K5nRz"
   },
   "source": [
    "### Library settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KRkatYS95mLP"
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l26gX-cHOFBu"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gT_OJBHfCg4Q"
   },
   "outputs": [],
   "source": [
    "def generate_multiturn_history(df: pd.DataFrame):\n",
    "    \"\"\"Processes a DataFrame of messages to add conversation history for each message.\n",
    "\n",
    "    This function takes a DataFrame containing message data and iterates through each row.\n",
    "    For each message in a row, it constructs the conversation history up to that point by\n",
    "    accumulating previous user and AI messages. This conversation history is then added\n",
    "    to the message data, and the processed messages are returned as a new DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A DataFrame containing message data. It is expected to have a column named\n",
    "            \"messages\" where each entry is a list of dictionaries representing messages in\n",
    "            a conversation. Each message dictionary should have \"user\" and \"reference\" keys.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with the processed messages. Each message dictionary will now have an\n",
    "        additional \"conversation_history\" key containing a list of tuples representing the\n",
    "        conversation history leading up to that message. The tuples are of the form\n",
    "        (\"user\", message_text) or (\"ai\", message_text).\n",
    "    \"\"\"\n",
    "    processed_messages = []\n",
    "    for i, row in df.iterrows():\n",
    "        conversation_history = []\n",
    "        for message in row[\"messages\"]:\n",
    "            message[\"conversation_history\"] = conversation_history\n",
    "            processed_messages.append(message)\n",
    "            conversation_history = conversation_history + [\n",
    "                (\"user\", message[\"user\"]),\n",
    "                (\"ai\", message[\"reference\"]),\n",
    "            ]\n",
    "    return pd.DataFrame(processed_messages)\n",
    "\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"\"\"Creates an iterable with tuples paired together\n",
    "    e.g s -> (s0, s1), (s2, s3), (s4, s5), ...\n",
    "    \"\"\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "\n",
    "def batch_generate_message(row: dict, callable: Any) -> dict:\n",
    "    \"\"\"\n",
    "    Predicts a response from a chat agent.\n",
    "\n",
    "    Args:\n",
    "        callable (ChatAgent): A chat agent.\n",
    "        row (dict): A message.\n",
    "    Returns:\n",
    "        dict: The predicted response.\n",
    "    \"\"\"\n",
    "    index, message = row\n",
    "\n",
    "    messages = []\n",
    "    for user_message, ground_truth in pairwise(message.get(\"conversation_history\", [])):\n",
    "        messages.append((\"user\", user_message))\n",
    "        messages.append((\"ai\", ground_truth))\n",
    "    messages.append((\"user\", message[\"user\"]))\n",
    "    input_callable = {\"messages\": messages, **message.get(\"callable_kwargs\", {})}\n",
    "    response = callable.invoke(input_callable)\n",
    "    message[\"response\"] = response.content\n",
    "    message[\"response_obj\"] = response\n",
    "    return message\n",
    "\n",
    "\n",
    "def batch_generate_messages(\n",
    "    messages: pd.DataFrame, callable: Any, max_workers: int = 4\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates AI-powered responses to a series of user messages using a provided callable.\n",
    "\n",
    "    This function efficiently processes a Pandas DataFrame containing user-AI message pairs,\n",
    "     utilizing the specified callable (either a LangChain Chain or a custom class with an\n",
    "     `invoke` method) to predict AI responses in parallel.\n",
    "\n",
    "    Args:\n",
    "        callable (callable): A callable object (e.g., LangChain Chain, custom class) used\n",
    "            for response generation. Must have an `invoke(messages: dict) ->\n",
    "            langchain_core.messages.ai.AIMessage` method.\n",
    "            The `messages` dict follows this structure:\n",
    "            {\"messages\" [(\"user\", \"first\"),(\"ai\", \"a response\"), (\"user\", \"a follow up\")]}\n",
    "\n",
    "        messages (pd.DataFrame): A DataFrame with one column named 'messages' containing\n",
    "            the list of user-AI message pairs as described above.\n",
    "\n",
    "        max_workers (int, optional): The number of worker processes to use for parallel\n",
    "            prediction. Defaults to the number of available CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the original messages and a new column with the predicted AI responses.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        messages_df = pd.DataFrame({\n",
    "            \"messages\": [\n",
    "                [{\"user\": \"What's the weather today?\", \"reference\": \"It's sunny.\"}],\n",
    "                [{\"user\": \"Tell me a joke.\", \"reference\": \"Why did the scarecrow win an award?...\n",
    "                    Because he was outstanding in his field!\"}]\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        responses_df = batch_generate_messages(my_callable, messages_df)\n",
    "        ```\n",
    "    \"\"\"\n",
    "    logging.info(\"Executing batch scoring\")\n",
    "    predicted_messages = []\n",
    "    with ThreadPoolExecutor(max_workers) as pool:\n",
    "        partial_func = partial(batch_generate_message, callable=callable)\n",
    "        for message in tqdm(\n",
    "            pool.map(partial_func, messages.iterrows()), total=len(messages)\n",
    "        ):\n",
    "            predicted_messages.append(message)\n",
    "    return pd.DataFrame(predicted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZMXiJhKD2TY"
   },
   "source": [
    "## Import ground truth data for evaluation\n",
    "\n",
    "In this sample, we will use 2 conversations as a ground truth. Every message in the conversations, along with the relative history, will be used to produce a response with a foundational model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jwqlusT7OJ4J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chats.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile chats.yaml\n",
    "- messages:\n",
    "  - user: Hi\n",
    "    reference: Hi, how can I help you?\n",
    "  - user: I'm looking for a recipe for a healthy dinner. Do you have any recommendations?\n",
    "    reference: Sure, I can help you with that. What are your dietary restrictions? Are you vegetarian, vegan, gluten-free, or anything else?\n",
    "  - user: I'm not vegetarian or vegan, but I am gluten-free.\n",
    "    reference: 'Okay, I ll keep that in mind. Here are a few recipes that I think you might like:\n",
    "  * **Grilled Salmon with Roasted Vegetables:** This is a delicious and healthy recipe that is perfect for a weeknight meal. The salmon is grilled to perfection and the roasted vegetables add a touch of sweetness.\n",
    "  * **Chicken Stir-Fry:** This is a quick and easy stir-fry that is perfect for busy weeknights. The chicken is cooked with vegetables and a light sauce.\n",
    "  * **Lentil Soup:** This is a hearty and healthy soup that is perfect for a cold winter day. The lentils are packed with protein and fiber, and the soup is also gluten-free.'\n",
    "  - user: Those all sound great! I think I'm going to try the grilled salmon with roasted vegetables.\n",
    "    reference: That's a great choice! I hope you enjoy it.\n",
    "  - user: Thanks for your help!\n",
    "    reference: You're welcome! Is there anything else I can help you with today?\n",
    "  - user: No, that's all. Thanks again!\n",
    "    reference: You're welcome! Have a great day!\n",
    "- messages:\n",
    "  - user: Hi\n",
    "    reference: Hi, how can I help you?\n",
    "  - user: I'm looking for a recipe for a romantic dinner. Do you have any recommendations?\n",
    "    reference: Sure, I can help you with that. What are your dietary restrictions? Are you vegetarian, vegan, gluten-free, or anything else?\n",
    "  - user: I'm vegetarian.\n",
    "    reference: 'Sure, I can help you find a healthy vegetarian dinner recipe. Here are a few ideas:\n",
    "    * **Burnt aubergine veggie chilli:** This is a hearty and flavorful dish that is packed with nutrients. The roasted aubergine gives it a smoky flavor, and the lentils and beans add protein and fiber.\n",
    "  * **Simple mushroom curry:** This is a quick and easy curry that is perfect for a weeknight meal. The mushrooms are cooked in a creamy sauce with spices, and the whole dish is ready in under 30 minutes.\n",
    "  * **Vegetarian enchiladas:** This is a classic Mexican dish that is easy to make vegetarian. The enchiladas are filled with a variety of vegetables, and they are topped with a delicious sauce.\n",
    "  * **Braised sesame tofu:** This is a flavorful and satisfying dish that is perfect for a cold night. The tofu is braised in a sauce with sesame, ginger, and garlic, and it is served over rice or noodles.\n",
    "  * **Roast garlic & tahini spinach:** This is a light and healthy dish that is perfect for a spring or summer meal. The spinach is roasted with garlic and tahini, and it is served with a side of pita bread.\n",
    "\n",
    "  These are just a few ideas to get you started. There are many other great vegetarian dinner recipes out there, so you are sure to find something that you will enjoy.'\n",
    "  - user: Those all sound great! I like the Burnt aubergine veggie chilli\n",
    "    reference: That's a great choice! I hope you enjoy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie7KTd_OjwdE"
   },
   "source": [
    "Let's now load all the messages into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j0eSy_avGCW7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'user': 'Hi', 'reference': 'Hi, how can I he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'user': 'Hi', 'reference': 'Hi, how can I he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages\n",
       "0  [{'user': 'Hi', 'reference': 'Hi, how can I he...\n",
       "1  [{'user': 'Hi', 'reference': 'Hi, how can I he..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = yaml.safe_load(open(\"chats.yaml\"))\n",
    "df = pd.DataFrame(y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA7G0bDTj84z"
   },
   "source": [
    "**Decomposing the chat message input/output pairs**\n",
    "\n",
    "We are now ready for decomposing multi-turn history. This is essential to enable batch prediction.\n",
    "\n",
    "We decompose the `messages` list into single input/output pairs. The input is always composed by `user message`, `reference message` and `conversation_history`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Given the following chat:\n",
    "\n",
    "```yaml\n",
    "- messages:\n",
    "- user: Hi\n",
    "reference: Hi, how can I help you?\n",
    "- user: I'm looking for a recipe for a healthy dinner. Do you have any recommendations?\n",
    "reference: Sure, I can help you with that. What are your dietary restrictions? Are you vegetarian, vegan, gluten-free, or anything else?\n",
    "```\n",
    "\n",
    "We can generate these two input/output samples:\n",
    "\n",
    "```yaml\n",
    "- user: Hi\n",
    "  reference: Hi, how can I help you?\n",
    "  conversation_history: []\n",
    "\n",
    "- user: I'm looking for a recipe for a healthy dinner....\n",
    "  reference: Sure, I can help you with that. What are your ...\n",
    "  conversation_history:\n",
    "  - user: Hi\n",
    "    reference: Hi, how can I help you?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1FL0R3YXPj3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>reference</th>\n",
       "      <th>conversation_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi</td>\n",
       "      <td>Hi, how can I help you?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm looking for a recipe for a healthy dinner....</td>\n",
       "      <td>Sure, I can help you with that. What are your ...</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not vegetarian or vegan, but I am gluten-f...</td>\n",
       "      <td>Okay, I ll keep that in mind. Here are a few r...</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Those all sound great! I think I'm going to tr...</td>\n",
       "      <td>That's a great choice! I hope you enjoy it.</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks for your help!</td>\n",
       "      <td>You're welcome! Is there anything else I can h...</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No, that's all. Thanks again!</td>\n",
       "      <td>You're welcome! Have a great day!</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hi</td>\n",
       "      <td>Hi, how can I help you?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm looking for a recipe for a romantic dinner...</td>\n",
       "      <td>Sure, I can help you with that. What are your ...</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm vegetarian.</td>\n",
       "      <td>Sure, I can help you find a healthy vegetarian...</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Those all sound great! I like the Burnt auberg...</td>\n",
       "      <td>That's a great choice! I hope you enjoy it.</td>\n",
       "      <td>[(user, Hi), (ai, Hi, how can I help you?), (u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user  \\\n",
       "0                                                 Hi   \n",
       "1  I'm looking for a recipe for a healthy dinner....   \n",
       "2  I'm not vegetarian or vegan, but I am gluten-f...   \n",
       "3  Those all sound great! I think I'm going to tr...   \n",
       "4                              Thanks for your help!   \n",
       "5                      No, that's all. Thanks again!   \n",
       "6                                                 Hi   \n",
       "7  I'm looking for a recipe for a romantic dinner...   \n",
       "8                                    I'm vegetarian.   \n",
       "9  Those all sound great! I like the Burnt auberg...   \n",
       "\n",
       "                                           reference  \\\n",
       "0                            Hi, how can I help you?   \n",
       "1  Sure, I can help you with that. What are your ...   \n",
       "2  Okay, I ll keep that in mind. Here are a few r...   \n",
       "3        That's a great choice! I hope you enjoy it.   \n",
       "4  You're welcome! Is there anything else I can h...   \n",
       "5                  You're welcome! Have a great day!   \n",
       "6                            Hi, how can I help you?   \n",
       "7  Sure, I can help you with that. What are your ...   \n",
       "8  Sure, I can help you find a healthy vegetarian...   \n",
       "9        That's a great choice! I hope you enjoy it.   \n",
       "\n",
       "                                conversation_history  \n",
       "0                                                 []  \n",
       "1        [(user, Hi), (ai, Hi, how can I help you?)]  \n",
       "2  [(user, Hi), (ai, Hi, how can I help you?), (u...  \n",
       "3  [(user, Hi), (ai, Hi, how can I help you?), (u...  \n",
       "4  [(user, Hi), (ai, Hi, how can I help you?), (u...  \n",
       "5  [(user, Hi), (ai, Hi, how can I help you?), (u...  \n",
       "6                                                 []  \n",
       "7        [(user, Hi), (ai, Hi, how can I help you?)]  \n",
       "8  [(user, Hi), (ai, Hi, how can I help you?), (u...  \n",
       "9  [(user, Hi), (ai, Hi, how can I help you?), (u...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = generate_multiturn_history(df)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OhQLR1lGsEq"
   },
   "source": [
    "## Let's define our LangChain chain!\n",
    "\n",
    "We now need to define our LangChain Chain. For this tutorial, we will create a simple conversational chain capable of producing cooking recipes for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u5B0ufczjfA_"
   },
   "outputs": [],
   "source": [
    "base_url = \"http://localhost:11434\"\n",
    "model = \"phi3\"\n",
    "\n",
    "llm = ChatOllama(base_url=base_url,\n",
    "                 model=model,\n",
    "                 temperature=0.1\n",
    "                 )\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a conversational bot that produce nice recipes for users based on a question.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS9rlK6eq5qY"
   },
   "source": [
    "We can test our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qzMotN92qrqv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm excited to help you with your culinary adventure today. What delicious dish can we create together? Feel free to share any dietary preferences or restrictions, and let’s get cooking!\", additional_kwargs={}, response_metadata={'model': 'phi3', 'created_at': '2024-11-20T14:11:38.939303Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2964087300, 'load_duration': 2439933000, 'prompt_eval_count': 36, 'prompt_eval_duration': 61273000, 'eval_count': 50, 'eval_duration': 456565000}, id='run-eaccb318-c91e-41eb-9d43-c8b0d480ac22-0', usage_metadata={'input_tokens': 36, 'output_tokens': 50, 'total_tokens': 86})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke([(\"human\", \"Hi there!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqdVpTMkq_pc"
   },
   "source": [
    "## Batch scoring\n",
    "\n",
    "We are now ready to perform batch scoring. To perform batch scoring we will leverage the utility function `batch_generate_messages`\n",
    "\n",
    "Have a look at the definition to see the expected input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qc4DiVYOKk6H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_generate_messages in module __main__:\n",
      "\n",
      "batch_generate_messages(messages: pandas.core.frame.DataFrame, callable: Any, max_workers: int = 4) -> pandas.core.frame.DataFrame\n",
      "    Generates AI-powered responses to a series of user messages using a provided callable.\n",
      "    \n",
      "    This function efficiently processes a Pandas DataFrame containing user-AI message pairs,\n",
      "     utilizing the specified callable (either a LangChain Chain or a custom class with an\n",
      "     `invoke` method) to predict AI responses in parallel.\n",
      "    \n",
      "    Args:\n",
      "        callable (callable): A callable object (e.g., LangChain Chain, custom class) used\n",
      "            for response generation. Must have an `invoke(messages: dict) ->\n",
      "            langchain_core.messages.ai.AIMessage` method.\n",
      "            The `messages` dict follows this structure:\n",
      "            {\"messages\" [(\"user\", \"first\"),(\"ai\", \"a response\"), (\"user\", \"a follow up\")]}\n",
      "    \n",
      "        messages (pd.DataFrame): A DataFrame with one column named 'messages' containing\n",
      "            the list of user-AI message pairs as described above.\n",
      "    \n",
      "        max_workers (int, optional): The number of worker processes to use for parallel\n",
      "            prediction. Defaults to the number of available CPU cores.\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: A DataFrame containing the original messages and a new column with the predicted AI responses.\n",
      "    \n",
      "    Example:\n",
      "        ```python\n",
      "        messages_df = pd.DataFrame({\n",
      "            \"messages\": [\n",
      "                [{\"user\": \"What's the weather today?\", \"reference\": \"It's sunny.\"}],\n",
      "                [{\"user\": \"Tell me a joke.\", \"reference\": \"Why did the scarecrow win an award?...\n",
      "                    Because he was outstanding in his field!\"}]\n",
      "            ]\n",
      "        })\n",
      "    \n",
      "        responses_df = batch_generate_messages(my_callable, messages_df)\n",
      "        ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(batch_generate_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Pip0WjopXMsu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:29,  3.33s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scored_data \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_generate_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m scored_data\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36mbatch_generate_messages\u001b[1;34m(messages, callable, max_workers)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m    107\u001b[0m     partial_func \u001b[38;5;241m=\u001b[39m partial(batch_generate_message, \u001b[38;5;28mcallable\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcallable\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    109\u001b[0m         pool\u001b[38;5;241m.\u001b[39mmap(partial_func, messages\u001b[38;5;241m.\u001b[39miterrows()), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(messages)\n\u001b[0;32m    110\u001b[0m     ):\n\u001b[0;32m    111\u001b[0m         predicted_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(predicted_messages)\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\concurrent\\futures\\_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\concurrent\\futures\\_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mbatch_generate_message\u001b[1;34m(row, callable)\u001b[0m\n\u001b[0;32m     57\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     58\u001b[0m input_callable \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmessage\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallable_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})}\n\u001b[1;32m---> 59\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_callable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     61\u001b[0m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_obj\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_ollama\\chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[0;32m    645\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    646\u001b[0m     )\n\u001b[0;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[0;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[0;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[0;32m    655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_ollama\\chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    547\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[0;32m    548\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[0;32m    549\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    562\u001b[0m                 ),\n\u001b[0;32m    563\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_ollama\\chat_models.py:505\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    501\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    502\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    504\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m--> 505\u001b[0m     ollama_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_messages_to_ollama_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m     stop \u001b[38;5;241m=\u001b[39m stop \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop\n\u001b[0;32m    509\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params\n",
      "File \u001b[1;32mc:\\Users\\leand\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_ollama\\chat_models.py:412\u001b[0m, in \u001b[0;36mChatOllama._convert_messages_to_ollama_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content_part \u001b[38;5;129;01min\u001b[39;00m cast(List[Dict], message\u001b[38;5;241m.\u001b[39mcontent):\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcontent_part\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    413\u001b[0m             content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontent_part[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m content_part\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_use\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "scored_data = batch_generate_messages(messages=df_processed, callable=chain)\n",
    "scored_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Tf1bssiSBk7"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We'll utilize [Vertex AI Rapid Evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/rapid-evaluation) to assess our generative AI model's performance. This service within Vertex AI streamlines the evaluation process, integrates with [Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments) for tracking, and offers a range of [pre-built metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#task-and-metrics) and the capability to define custom ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZSzjLI_rBSp"
   },
   "source": [
    "#### Define a CustomMetric using Gemini model\n",
    "\n",
    "Define a customized Gemini model-based metric function, with explanations for the score. The registered custom metrics are computed on the client side, without using online evaluation service APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT0uclHrSBlC"
   },
   "outputs": [],
   "source": [
    "evaluator_llm = ChatOllama(\n",
    "    model_name=\"phi3\",\n",
    "    temperature=0,\n",
    "    response_mime_type=\"application/json\",\n",
    ")\n",
    "\n",
    "\n",
    "def custom_faithfulness(instance):\n",
    "    prompt = f\"\"\"You are examining written text content. Here is the text:\n",
    "************\n",
    "Written content: {instance[\"response\"]}\n",
    "************\n",
    "Original source data: {instance[\"reference\"]}\n",
    "\n",
    "Examine the text and determine whether the text is faithful or not.\n",
    "Faithfulness refers to how accurately a generated summary reflects the essential information and key concepts present in the original source document.\n",
    "A faithful summary stays true to the facts and meaning of the source text, without introducing distortions, hallucinations, or information that wasn't originally there.\n",
    "\n",
    "Your response must be an explanation of your thinking along with single integer number on a scale of 0-5, 0\n",
    "the least faithful and 5 being the most faithful.\n",
    "\n",
    "Produce results in JSON\n",
    "\n",
    "Expected format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"explanation\": \"< your explanation>\",\n",
    "    \"custom_faithfulness\": <your score>\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    result = evaluator_llm.invoke([(\"human\", prompt)])\n",
    "    result = json.loads(result.content)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Register Custom Metric\n",
    "custom_faithfulness_metric = CustomMetric(\n",
    "    name=\"custom_faithfulness\",\n",
    "    metric_function=custom_faithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpdRqYlLq53t"
   },
   "source": [
    "### Run Evaluation with CustomMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4iU_mIhoY93"
   },
   "outputs": [],
   "source": [
    "experiment_name = \"rapid-eval-langchain-eval\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3r3NiKNqn3u"
   },
   "source": [
    "We are now ready to run the evaluation. We will use different metrics, combining the custom metric we defined above with some pre-built metrics.\n",
    "\n",
    "Results of the evaluation will be automatically tagged into the experiment_name we defined.\n",
    "\n",
    "You can click `View Experiment`, to see the experiment in Google Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeVra-g1rAFV"
   },
   "outputs": [],
   "source": [
    "metrics = [\"fluency\", \"coherence\", \"safety\", custom_faithfulness_metric]\n",
    "\n",
    "eval_task = EvalTask(\n",
    "    dataset=scored_data,\n",
    "    metrics=metrics,\n",
    "    experiment=experiment_name,\n",
    "    metric_column_mapping={\"prompt\": \"user\"},\n",
    ")\n",
    "eval_result = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka3tZCL_uurD"
   },
   "source": [
    "Once an eval result is produced, we are able to display summary metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KheOvIvtiRlz"
   },
   "outputs": [],
   "source": [
    "eval_result.summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcALGGlwu0p_"
   },
   "source": [
    "We are also able to display a pandas dataframe containing a detailed summary of how our eval dataset performed and relative granular metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zJ686YYiWJC"
   },
   "outputs": [],
   "source": [
    "eval_result.metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1NsUKA3vEu8"
   },
   "source": [
    "## Iterating over the prompt\n",
    "\n",
    "Let's perform some simple changes to our chain to see how our evaluation results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_8PnvLSv7Nu"
   },
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a conversational bot that produce nice recipes for users based on a question.\n",
    "Before suggesting a recipe, you should ask for the dietary requirements..\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_chain = template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JF_Po81twPbr"
   },
   "outputs": [],
   "source": [
    "scored_data = batch_generate_messages(messages=df_processed, callable=new_chain)\n",
    "scored_data.rename(columns={\"text\": \"response\"}, inplace=True)\n",
    "scored_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snIQ0itfwUZa"
   },
   "outputs": [],
   "source": [
    "metrics = [\"fluency\", \"coherence\", \"safety\", custom_faithfulness_metric]\n",
    "eval_task = EvalTask(\n",
    "    dataset=scored_data,\n",
    "    metrics=metrics,\n",
    "    experiment=experiment_name,\n",
    "    metric_column_mapping={\"prompt\": \"user\"},\n",
    ")\n",
    "eval_result = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT6Ma5FLwfbF"
   },
   "outputs": [],
   "source": [
    "eval_result.summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b42l5juJsyyY"
   },
   "source": [
    "#### Let's compare both eval results\n",
    "\n",
    "We can do that by using the method `display_runs` for a given `eval task` object to see which prompt template performed best on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP0Zcm1yvh95"
   },
   "outputs": [],
   "source": [
    "df = vertexai.preview.get_experiment_df(experiment_name).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete Experiments\n",
    "delete_experiments = True\n",
    "if delete_experiments or os.getenv(\"IS_TESTING\"):\n",
    "    experiments_list = aiplatform.Experiment.list()\n",
    "    for experiment in experiments_list:\n",
    "        experiment.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "evaluate_langchain_chains.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
