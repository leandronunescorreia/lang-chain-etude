{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Unstructure Data from Office files\n",
    "```bash\n",
    "pip install unstructured openpyxl python-magic python-pptx\n",
    "pip install langchain-unstructured\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "import textwrap\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPowerPointLoader(\"docs\\\\xLLM.pptx\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '\\n\\n1\\n\\nMLtechniques.com – xLLM, by Vincent Granville\\n\\nFast, High Performance Multi-LLM System Without Neural Network\\n\\nxLLM: New Generation of Large Language Models\\x0b\\x0bVincent Granville, PhD\\x0bChief AI Scientist \\x0bGenAItechLab.com\\x0bvincentg@mltechniques.com\\x0b \\x0bMay 29, 2024\\n\\n',\n",
       " 2: '\\n\\n2\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtreme LLM (xLLM) in a Snapshot\\n\\nMixture of experts\\n\\nOne specialized, small sub-LLM per top category\\n\\nLLM router (multi-agent system) to manage the sub-LLMs\\n\\nUser selects top category and hyperparameters\\n\\nEach sub-LLM built with its own taxonomy and knowledge graph\\n\\nNo neural network, no training\\n\\nThus, low cost, easy to fine-tune (in milliseconds per sub-LLM)\\n\\nSelf-tuned based on favorite hyperparameters, and customizable\\n\\nNo GPU, no latency, better results, local implementation\\n\\n',\n",
       " 3: '\\n\\n3\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtreme LLM (xLLM) in a Snapshot\\n\\nConcise results\\n\\nMultiple sections displayed to user: links, related content, x-embeddings\\n\\nOutput with relevancy score attached to each item in each section\\n\\nSupports bulk queries, output saved to file\\n\\nGreat for search, targeted to busy professional users and experts\\n\\nCase studies\\n\\nCorporate corpus with augmented sources (content + taxonomies)\\n\\nWolfram corpus: 15 sub-LLMs, 500 sub-categories per sub-LLM\\n\\nPublisher, 4000 titles: clustering, predicting article performance\\n\\n',\n",
       " 4: '\\n\\n4\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Architecture\\n\\n',\n",
       " 5: '\\n\\n5\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nOutput: KW = Hypothesis, Sub-LLM = Stats\\n\\n',\n",
       " 6: '\\n\\n6\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components\\n\\nSmart crawling to retrieve embedded structure\\n\\nBreadcrumbs (enterprise corpus), concept associations (related links)\\n\\nMetadata, tags, taxonomy (category graph)\\n\\nAugmented with user prompts\\n\\nAugmented with books (TOC, index, glossaries, synonyms, titles)\\n\\nX-embeddings\\n\\nVariable-length embeddings stored as sparse nested hashes\\n\\nMulti-token: “data~science” on top of single tokens “data” and “science”\\n\\nContextual token: “data^science”, both words in same paragraph but not adjacent \\n\\nPMI (pointwise mutual information) instead of dot product / cosine distance\\n\\nParametric weights attached to tokens (no loss function to optimize)\\n\\n',\n",
       " 7: '\\n\\n7\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtract from Taxonomy Graph\\n\\n',\n",
       " 8: '\\n\\n8\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtract from Embedding Table\\n\\n',\n",
       " 9: '\\n\\n9\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components (Cont.)\\n\\nHome-made libraries\\n\\nIssues with Python libraries (singularize, autocorrect, “Feller” changed to “seller”) \\n\\nMinimize stemming and text transforms; keep plural if found in corpus \\n\\nImportant: accented characters, separators (punctuation), capital letters\\n\\nAd-hoc lists: home-made stopwords, do not singularize, do not autocorrect\\n\\nBackend tables (specific to each sub-LLM)\\n\\nX-embeddings not the most important table; taxonomy more important\\n\\nCompressed n-grams: key to n-gram table is ordered n-gram (“learning machine”); value = list of n-gram permutations found in corpus (“machine learning”)\\n\\n',\n",
       " 10: '\\n\\n10\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSample Entry from Taxonomy Table \\n\\nExample:\\n\\ncentral~limit {‘Lindeberg Condition | Limit Theorems | 4’ : 1, ‘Lyapunov Condition | Limit Theorems | 4’ : 2}\\n\\nExplanations:\\n\\nKey: “central~limit”, a word found in the corpus\\n\\nValue: hash table with comma-separated key-value pairs (nested key-value)\\n\\nNested key has 3 elements: category, parent category, and category level for parent category (depth); these elements are separated by the | symbol\\n\\nThe value attached to a nested key, after the colon symbol, is the number of times the word in question is found on a webpage with the category in question\\n\\n',\n",
       " 11: '\\n\\n11\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components (Cont.)\\n\\nFast Approximate Nearest Neighbor Search\\n\\nWith caching, variable-length embeddings, and compressed n-grams for faster search\\n\\nGoal: find embeddings in backend tables, similar to those detected in user prompt\\n\\nSynonyms dictionary and abbreviations map. Built on glossaries, indexes, etc. For instance, map “ANOVA” to “Analysis of Variance”\\n\\nLLM Router. Example: “gradient descent” prompt word not found in the “Statistics” sub-LLM; reroute to the “Calculus” sub-LLM to retrieve items relevant to the prompt. \\n\\nAugmented taxonomy\\n\\nWhen crawled taxonomy is poor, use external taxonomy or build one from scratch\\n\\nUse taxonomy entries as regular text in embeddings, with higher weight\\n\\n',\n",
       " 12: '\\n\\n12\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFast Approximate Nearest Neighbor Search\\n\\nRed dot: user-derived embedding\\n\\nBlue dot: backend table embedding\\n\\nOver time, arrows link red dots to their nearest blue dots\\n\\n',\n",
       " 13: '\\n\\n13\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nEvaluation\\n\\nUser-based (automated)\\n\\nCollect favorite hyperparameters chosen by users\\n\\nUse smart grid search to set default hyperparameters based on user favorites \\n\\nFine-tune on one or few sub-LLMs (like LoRA) before full optimization on (say) 200 sub-LLMs. You may fine-tune all sub-LLMs in parallel.   \\n\\nTaxonomy-based (automated)\\n\\nPretend that the taxonomy backend table comes from external sources\\n\\nAssign categories to webpages based on this “external” taxonomy \\n\\nFor each webpage, compare externally assigned to native category\\n\\n',\n",
       " 14: '\\n\\n14\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nTaxonomy-Based Evaluation \\n\\n',\n",
       " 15: '\\n\\n15\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions\\n\\nCase study\\n\\nPredicting article pageviews based on title keywords and category\\n\\n4000 articles; pageview is normalized and time-adjusted\\n\\nEvaluation and Loss function (identical)\\n\\nBased on comparing predicted with observed quantiles \\n\\nUsing 5 quantiles (see code)\\n\\nGood proxy to Kolmogorov-Smirnov distance\\n\\n',\n",
       " 16: '\\n\\n16\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions – Model\\n\\n',\n",
       " 17: '\\n\\n17\\n\\nMLtechniques.com - NoGAN Synthesizer, by Vincent Granville\\n\\nSmart Encoding of Categorical Features \\n\\nCreate new codes sequentially as you browse the training set. \\n\\nAggregate codes with few observations into bundles.\\n\\nCreate two key-value mappings. Ex:\\n\\nCategory_to_Code[‘Blog’, ‘William’] = 5\\n\\nCode_to_category[5] = [‘Blog’, ‘William’]\\n\\nReplace the categorical features by the newly created feature, “Code”.\\n\\nNumber of codes ≤ number of obs. \\n\\n',\n",
       " 18: '\\n\\n18\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions – Results \\n\\nScatterplot, observed vs predicted pv\\n\\nPredicted pv (pageview) adjusted due to small prediction bias\\n\\n',\n",
       " 19: '\\n\\n19\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Clustering\\n\\nCase study\\n\\nIdentifying patterns / clusters in popular articles based on title keywords\\n\\n4000 articles; pageview is normalized and time-adjusted\\n\\nMethodology\\n\\nGroup multi-tokens into clusters based on a similarity metric, with hierarchical clustering and k-medoids\\n\\nLet S(t) be the set of articles containing the multi-token t in the title\\n\\nFor each multi-token group G, the list L(G) of articles belonging to G is\\n\\n',\n",
       " 20: '\\n\\n20\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Clustering\\n\\nSimilarity between two multi-tokens t1, t2\\n\\nRemarks\\n\\nMulti-token clusters are non-overlapping, but article clusters may overlap\\n\\nSklearn clustering methods require a distance matrix as input; the matrix (derived from the similarity metric) is huge but extremely sparse.\\n\\nIn my implementation, s(t1, t2) is computed and stored only if it is strictly positive. Using connected components for clustering, it is far more efficient than Sklearn.\\n\\n',\n",
       " 21: '\\n\\n21\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nDendrogram for Multi-Token Clusters\\n\\n',\n",
       " 22: '\\n\\n22\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSample Cluster of Popular Articles\\n\\nLinked to multi-token cluster with 3 elements, including one contextual multi-token: “Machine^vs” (pv stands for normalized pageview)\\n\\n',\n",
       " 23: '\\n\\n23\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFuture Research\\n\\nUsing model evaluation metric as loss function\\n\\nGoal: increase quality, as standard loss functions are poor proxies to model evaluation\\n\\nFeasible for supervised learning (predictions), not unsupervised (LLM, clustering)\\n\\nChallenge: computationally impossible for good evaluation metrics such as multivariate Kolmogorov-Smirnov distance (KS)\\n\\nSolution:  use Hellinger distance as loss function; start with rough approximation that increases in granularity and converges to evaluation metric (Hellinger) over time\\n\\nThus, loss function changes over time; called adaptive loss function.  \\n\\nUnlike KS, easy to make billions of atomic changes in Hellinger distance (weight updates or neuron activations). Useful when dealing with billions of weights or tokens.\\n\\n',\n",
       " 24: '\\n\\n24\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFuture Research – Adaptive Loss Function\\n\\nRefine loss function when it stops decreasing to avoid getting stuck in local optimum: see picture, with 10 change points (Y-axis = loss, X-axis = iterations)\\n\\n',\n",
       " 25: '\\n\\n25\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nAdaptive Loss Function – Example \\n\\nTabular Data Synthetization\\n\\nReal data: 2 concentric circles\\n\\nSynthesized, NoGAN synthesizer: blue dots. Constrained synthetization to keep loss above some threshold\\n\\nAs the loss function gets more granular, the synthesized data gets more similar to the real data (the training set)\\n\\n',\n",
       " 26: '\\n\\n26\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFinal Notes\\n\\nClassic LLMs (powered by deep neural networks) \\n\\nBillions of weights, most never used to answer a prompt\\n\\nWaste of resources, lots of noise in tokens, may produce hallucinations\\n\\nUser pays by the token: expensive\\n\\nInitially used to predict next token in a sentence\\n\\nxLLM\\n\\nFast, no training, no latency, explainable AI, secure\\n\\nDoes not need prompt engineering\\n\\nNot relying on unpredictable Blackbox Python libraries\\n\\nOpen source, made from scratch, great for search, easy to fine-tune\\n\\n',\n",
       " 27: '\\n\\n27\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nReferences\\n\\nMain book: “State of the Art in GenAI & LLMs – Creative Projects, with Solutions”, available here.  \\n\\nProject 2.4 – Adaptive loss function\\n\\nProject 7.2 – Main part, includes smart crawling and x-embeddings\\n\\nProject 8.1 – Fast approximate nearest neighbor search\\n\\nProject 8.2 – Evaluation using taxonomy\\n\\nProject 8.3 – xLLM for clustering and predictions\\n\\nAppendix C3 & C4 – Comparing xLLM to classical LLMs\\n\\nOn GitHub\\n\\nxLLM repository, here. Includes code and backend tables.\\n\\nNewsletter and resources: sign up here.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_data = {}\n",
    "for doc in docs:\n",
    "    page = doc.metadata[\"page_number\"]\n",
    "    ppt_data[page] = ppt_data.get(page, \"\") + '\\n\\n' + doc.page_content\n",
    "\n",
    "ppt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### slide 1: \\n\\n1\\n\\nMLtechniques.com – xLLM, by Vincent Granville\\n\\nFast, High Performance Multi-LLM System Without Neural Network\\n\\nxLLM: New Generation of Large Language Models\\x0b\\x0bVincent Granville, PhD\\x0bChief AI Scientist \\x0bGenAItechLab.com\\x0bvincentg@mltechniques.com\\x0b \\x0bMay 29, 2024\\n\\n### slide 2: \\n\\n2\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtreme LLM (xLLM) in a Snapshot\\n\\nMixture of experts\\n\\nOne specialized, small sub-LLM per top category\\n\\nLLM router (multi-agent system) to manage the sub-LLMs\\n\\nUser selects top category and hyperparameters\\n\\nEach sub-LLM built with its own taxonomy and knowledge graph\\n\\nNo neural network, no training\\n\\nThus, low cost, easy to fine-tune (in milliseconds per sub-LLM)\\n\\nSelf-tuned based on favorite hyperparameters, and customizable\\n\\nNo GPU, no latency, better results, local implementation\\n\\n### slide 3: \\n\\n3\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtreme LLM (xLLM) in a Snapshot\\n\\nConcise results\\n\\nMultiple sections displayed to user: links, related content, x-embeddings\\n\\nOutput with relevancy score attached to each item in each section\\n\\nSupports bulk queries, output saved to file\\n\\nGreat for search, targeted to busy professional users and experts\\n\\nCase studies\\n\\nCorporate corpus with augmented sources (content + taxonomies)\\n\\nWolfram corpus: 15 sub-LLMs, 500 sub-categories per sub-LLM\\n\\nPublisher, 4000 titles: clustering, predicting article performance\\n\\n### slide 4: \\n\\n4\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Architecture\\n\\n### slide 5: \\n\\n5\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nOutput: KW = Hypothesis, Sub-LLM = Stats\\n\\n### slide 6: \\n\\n6\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components\\n\\nSmart crawling to retrieve embedded structure\\n\\nBreadcrumbs (enterprise corpus), concept associations (related links)\\n\\nMetadata, tags, taxonomy (category graph)\\n\\nAugmented with user prompts\\n\\nAugmented with books (TOC, index, glossaries, synonyms, titles)\\n\\nX-embeddings\\n\\nVariable-length embeddings stored as sparse nested hashes\\n\\nMulti-token: “data~science” on top of single tokens “data” and “science”\\n\\nContextual token: “data^science”, both words in same paragraph but not adjacent \\n\\nPMI (pointwise mutual information) instead of dot product / cosine distance\\n\\nParametric weights attached to tokens (no loss function to optimize)\\n\\n### slide 7: \\n\\n7\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtract from Taxonomy Graph\\n\\n### slide 8: \\n\\n8\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nExtract from Embedding Table\\n\\n### slide 9: \\n\\n9\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components (Cont.)\\n\\nHome-made libraries\\n\\nIssues with Python libraries (singularize, autocorrect, “Feller” changed to “seller”) \\n\\nMinimize stemming and text transforms; keep plural if found in corpus \\n\\nImportant: accented characters, separators (punctuation), capital letters\\n\\nAd-hoc lists: home-made stopwords, do not singularize, do not autocorrect\\n\\nBackend tables (specific to each sub-LLM)\\n\\nX-embeddings not the most important table; taxonomy more important\\n\\nCompressed n-grams: key to n-gram table is ordered n-gram (“learning machine”); value = list of n-gram permutations found in corpus (“machine learning”)\\n\\n### slide 10: \\n\\n10\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSample Entry from Taxonomy Table \\n\\nExample:\\n\\ncentral~limit {‘Lindeberg Condition | Limit Theorems | 4’ : 1, ‘Lyapunov Condition | Limit Theorems | 4’ : 2}\\n\\nExplanations:\\n\\nKey: “central~limit”, a word found in the corpus\\n\\nValue: hash table with comma-separated key-value pairs (nested key-value)\\n\\nNested key has 3 elements: category, parent category, and category level for parent category (depth); these elements are separated by the | symbol\\n\\nThe value attached to a nested key, after the colon symbol, is the number of times the word in question is found on a webpage with the category in question\\n\\n### slide 11: \\n\\n11\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSub-LLM Components (Cont.)\\n\\nFast Approximate Nearest Neighbor Search\\n\\nWith caching, variable-length embeddings, and compressed n-grams for faster search\\n\\nGoal: find embeddings in backend tables, similar to those detected in user prompt\\n\\nSynonyms dictionary and abbreviations map. Built on glossaries, indexes, etc. For instance, map “ANOVA” to “Analysis of Variance”\\n\\nLLM Router. Example: “gradient descent” prompt word not found in the “Statistics” sub-LLM; reroute to the “Calculus” sub-LLM to retrieve items relevant to the prompt. \\n\\nAugmented taxonomy\\n\\nWhen crawled taxonomy is poor, use external taxonomy or build one from scratch\\n\\nUse taxonomy entries as regular text in embeddings, with higher weight\\n\\n### slide 12: \\n\\n12\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFast Approximate Nearest Neighbor Search\\n\\nRed dot: user-derived embedding\\n\\nBlue dot: backend table embedding\\n\\nOver time, arrows link red dots to their nearest blue dots\\n\\n### slide 13: \\n\\n13\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nEvaluation\\n\\nUser-based (automated)\\n\\nCollect favorite hyperparameters chosen by users\\n\\nUse smart grid search to set default hyperparameters based on user favorites \\n\\nFine-tune on one or few sub-LLMs (like LoRA) before full optimization on (say) 200 sub-LLMs. You may fine-tune all sub-LLMs in parallel.   \\n\\nTaxonomy-based (automated)\\n\\nPretend that the taxonomy backend table comes from external sources\\n\\nAssign categories to webpages based on this “external” taxonomy \\n\\nFor each webpage, compare externally assigned to native category\\n\\n### slide 14: \\n\\n14\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nTaxonomy-Based Evaluation\\n\\n### slide 15: \\n\\n15\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions\\n\\nCase study\\n\\nPredicting article pageviews based on title keywords and category\\n\\n4000 articles; pageview is normalized and time-adjusted\\n\\nEvaluation and Loss function (identical)\\n\\nBased on comparing predicted with observed quantiles \\n\\nUsing 5 quantiles (see code)\\n\\nGood proxy to Kolmogorov-Smirnov distance\\n\\n### slide 16: \\n\\n16\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions – Model\\n\\n### slide 17: \\n\\n17\\n\\nMLtechniques.com - NoGAN Synthesizer, by Vincent Granville\\n\\nSmart Encoding of Categorical Features \\n\\nCreate new codes sequentially as you browse the training set. \\n\\nAggregate codes with few observations into bundles.\\n\\nCreate two key-value mappings. Ex:\\n\\nCategory_to_Code[‘Blog’, ‘William’] = 5\\n\\nCode_to_category[5] = [‘Blog’, ‘William’]\\n\\nReplace the categorical features by the newly created feature, “Code”.\\n\\nNumber of codes ≤ number of obs.\\n\\n### slide 18: \\n\\n18\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Predictions – Results \\n\\nScatterplot, observed vs predicted pv\\n\\nPredicted pv (pageview) adjusted due to small prediction bias\\n\\n### slide 19: \\n\\n19\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Clustering\\n\\nCase study\\n\\nIdentifying patterns / clusters in popular articles based on title keywords\\n\\n4000 articles; pageview is normalized and time-adjusted\\n\\nMethodology\\n\\nGroup multi-tokens into clusters based on a similarity metric, with hierarchical clustering and k-medoids\\n\\nLet S(t) be the set of articles containing the multi-token t in the title\\n\\nFor each multi-token group G, the list L(G) of articles belonging to G is\\n\\n### slide 20: \\n\\n20\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nxLLM for Clustering\\n\\nSimilarity between two multi-tokens t1, t2\\n\\nRemarks\\n\\nMulti-token clusters are non-overlapping, but article clusters may overlap\\n\\nSklearn clustering methods require a distance matrix as input; the matrix (derived from the similarity metric) is huge but extremely sparse.\\n\\nIn my implementation, s(t1, t2) is computed and stored only if it is strictly positive. Using connected components for clustering, it is far more efficient than Sklearn.\\n\\n### slide 21: \\n\\n21\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nDendrogram for Multi-Token Clusters\\n\\n### slide 22: \\n\\n22\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nSample Cluster of Popular Articles\\n\\nLinked to multi-token cluster with 3 elements, including one contextual multi-token: “Machine^vs” (pv stands for normalized pageview)\\n\\n### slide 23: \\n\\n23\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFuture Research\\n\\nUsing model evaluation metric as loss function\\n\\nGoal: increase quality, as standard loss functions are poor proxies to model evaluation\\n\\nFeasible for supervised learning (predictions), not unsupervised (LLM, clustering)\\n\\nChallenge: computationally impossible for good evaluation metrics such as multivariate Kolmogorov-Smirnov distance (KS)\\n\\nSolution:  use Hellinger distance as loss function; start with rough approximation that increases in granularity and converges to evaluation metric (Hellinger) over time\\n\\nThus, loss function changes over time; called adaptive loss function.  \\n\\nUnlike KS, easy to make billions of atomic changes in Hellinger distance (weight updates or neuron activations). Useful when dealing with billions of weights or tokens.\\n\\n### slide 24: \\n\\n24\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFuture Research – Adaptive Loss Function\\n\\nRefine loss function when it stops decreasing to avoid getting stuck in local optimum: see picture, with 10 change points (Y-axis = loss, X-axis = iterations)\\n\\n### slide 25: \\n\\n25\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nAdaptive Loss Function – Example \\n\\nTabular Data Synthetization\\n\\nReal data: 2 concentric circles\\n\\nSynthesized, NoGAN synthesizer: blue dots. Constrained synthetization to keep loss above some threshold\\n\\nAs the loss function gets more granular, the synthesized data gets more similar to the real data (the training set)\\n\\n### slide 26: \\n\\n26\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nFinal Notes\\n\\nClassic LLMs (powered by deep neural networks) \\n\\nBillions of weights, most never used to answer a prompt\\n\\nWaste of resources, lots of noise in tokens, may produce hallucinations\\n\\nUser pays by the token: expensive\\n\\nInitially used to predict next token in a sentence\\n\\nxLLM\\n\\nFast, no training, no latency, explainable AI, secure\\n\\nDoes not need prompt engineering\\n\\nNot relying on unpredictable Blackbox Python libraries\\n\\nOpen source, made from scratch, great for search, easy to fine-tune\\n\\n### slide 27: \\n\\n27\\n\\nMLtechniques.com - xLLM, by Vincent Granville\\n\\nReferences\\n\\nMain book: “State of the Art in GenAI & LLMs – Creative Projects, with Solutions”, available here.  \\n\\nProject 2.4 – Adaptive loss function\\n\\nProject 7.2 – Main part, includes smart crawling and x-embeddings\\n\\nProject 8.1 – Fast approximate nearest neighbor search\\n\\nProject 8.2 – Evaluation using taxonomy\\n\\nProject 8.3 – xLLM for clustering and predictions\\n\\nAppendix C3 & C4 – Comparing xLLM to classical LLMs\\n\\nOn GitHub\\n\\nxLLM repository, here. Includes code and backend tables.\\n\\nNewsletter and resources: sign up here.\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\n",
    "for page, content in ppt_data.items():\n",
    "    context += f\"### slide {page}: \\n\\n{content.strip()}\\n\\n\"\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = \"llama3.2:3b\"\n",
    "\n",
    "llm = ChatOllama(base_url=base_url,\n",
    "                 model=model,\n",
    "                 temperature=0.1,\n",
    "                 num_predict=256\n",
    "                 )\n",
    "\n",
    "system = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant who answer user question based on provide context.\n",
    "\"\"\")\n",
    "\n",
    "prompt = \"\"\"Answer user question based on the provided context ONLY! if do not know the answer, just sai \"I don't know\".\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = HumanMessagePromptTemplate.from_template(prompt)\n",
    "\n",
    "messages = [system, prompt]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "qna_chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(context, question):\n",
    "    return qna_chain.invoke({'context':context, 'question':question}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know how to generate content for PowerPoint slides based on the provided context. However, I can provide you with a general outline of what the key points might be for each slide:\n",
      "\n",
      "**Slide 1: Introduction**\n",
      "* Title: \"xLLM\"\n",
      "* Subtitle: \"A Novel Approach to Large Language Models\"\n",
      "* Key point: Introducing xLLM, a new method for large language models that improves upon existing approaches.\n",
      "\n",
      "**Slide 2: Problem Statement**\n",
      "* Title: \"Challenges in Large Language Models\"\n",
      "* Bullet points:\n",
      "\t+ Limited contextual understanding\n",
      "\t+ Inefficient use of resources\n",
      "\t+ Difficulty in handling out-of-vocabulary words\n",
      "* Key point: Highlighting the limitations and challenges of traditional large language models.\n",
      "\n",
      "**Slide 3: Solution Overview**\n",
      "* Title: \"xLLM: A Novel Approach\"\n",
      "* Bullet points:\n",
      "\t+ Adaptive loss function\n",
      "\t+ Smart crawling and x-embeddings\n",
      "\t+ Fast approximate nearest neighbor search\n",
      "* Key point: Introducing the key components of xLLM and how they address the challenges mentioned earlier.\n",
      "\n",
      "**Slide 4: Adaptive Loss Function**\n",
      "* Title: \"Adaptive Loss Function\"\n",
      "* Bullet points:\n",
      "\t+ Improves upon traditional loss functions\n",
      "\t+\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"For each PowerPoint slide provided above, write in MarkDown format the key points.\n",
    "Ensure smooth flow between slides, maintaining a clear and engaging narrative.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_llm(content, question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
